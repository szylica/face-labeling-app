
### Wyjaśnienie pliku tuner_ai_model_face_emotion.py

1. **Import bibliotek:**
   Importowane są niezbędne biblioteki do przetwarzania obrazów, budowy modelu, optymalizacji oraz logowania procesu trenowania.

2. **Ścieżki do danych:**
   Określono lokalizacje katalogów zawierających dane treningowe, walidacyjne i testowe.

3. **Parametry obrazu:**
   Obrazy są skalowane do wymiarów 150x150 pikseli, co jest zgodne z wymaganiami modelu VGG16. Batch size wynosi 64.

4. **Przygotowanie danych:**
   - **Augmentacja danych treningowych:**
     Dane są normalizowane (podzielone przez 255), a dodatkowo stosuje się augmentację, by zwiększyć różnorodność danych.
   - **Przygotowanie danych walidacyjnych:**
     Dane walidacyjne są tylko normalizowane.
   - **Generatory:**
     Generatory wczytują dane z odpowiednich katalogów i przygotowują je do treningu.

5. **Wczytanie modelu VGG16:**
   Wykorzystano pretrenowany model VGG16 (bez górnych warstw), aby skorzystać z już wytrenowanych cech.

6. **Zamrożenie wag:**
   Zablokowano warstwy modelu VGG16, by nie aktualizować ich podczas trenowania.

7. **Rozbudowa modelu:**
   Dodano nowe warstwy w pełni połączone, by dostosować model do klasyfikacji emocji. Ostatnia warstwa ma tyle neuronów, ile klas w danych treningowych.

8. **Kompilacja modelu:**
   Model jest kompilowany z optymalizatorem Adam, funkcją strat `categorical_crossentropy` i metryką `accuracy`.

9. **Callbacki:**
   - `EarlyStopping`: Zatrzymuje trenowanie, gdy wynik walidacyjny nie poprawia się przez 5 epok.
   - `ReduceLROnPlateau`: Redukuje współczynnik uczenia, gdy wynik walidacyjny nie poprawia się przez 3 epoki.
   - `TensorBoard`: Loguje informacje o treningu do wizualizacji.

10. **Trenowanie modelu:**
    Model jest trenowany przez maksymalnie 30 epok, wykorzystując dane treningowe i walidacyjne.

11. **Zapis modelu:**
    Wytrenowany model jest zapisywany jako plik `.h5`.
